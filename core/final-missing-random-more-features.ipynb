{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final\n",
    "Feature Engineering is the key.\n",
    "\n",
    "  \n",
    "reference:  \n",
    "https://www.kaggle.com/mjbahmani/statistical-analysis-for-elo  \n",
    "https://www.kaggle.com/c/elo-merchant-category-recommendation/discussion/82055  \n",
    "https://www.kaggle.com/chauhuynh/my-first-kernel-3-699  \n",
    "https://www.kaggle.com/fabiendaniel/elo-world  \n",
    "https://www.kaggle.com/raddar/target-true-meaning-revealed  \n",
    "https://www.kaggle.com/c/elo-merchant-category-recommendation/discussion/82036#479038  \n",
    "https://machinelearningmastery.com/basic-feature-engineering-time-series-data-python/  \n",
    "https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering this is really helpfull  \n",
    "https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc\n",
    "\n",
    "lightgbm params tuning:\n",
    "https://www.kaggle.com/garethjns/microsoft-lightgbm-with-parameter-tuning-0-823\n",
    "\n",
    "datetime feature  \n",
    "https://datascience.stackexchange.com/questions/27273/does-it-make-sense-that-datetime-encodes-one-hot-vector-like-one-hot-encoding-or"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan\n",
    "After the analysis of mid-term project, we achieved a performance (RMSE) at 3.6524 on validation dataset using lightGBM model. Now we need to think can we do better on it? So we are planning to conduct the analysis further:\n",
    "1. Can we get better score by using different or more features?\n",
    "2. Can we get better score by optimizing the model?\n",
    "3. Can we get better score by dealing with outliers more properly?\n",
    "4. we need to do CV.\n",
    "5. Xgboost/Catboost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data_Dictionary.xlsx', 'new_merchant_transactions.csv', 'test.csv', '~$Data_Dictionary.xlsx', 'merchants.csv', 'historical_transactions.csv', 'train.csv', '.ipynb_checkpoints', 'sample_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(4590)\n",
    "\n",
    "print(os.listdir(\"../data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...loaded\n"
     ]
    }
   ],
   "source": [
    "train_raw = pd.read_csv('../data/train.csv')\n",
    "test_raw = pd.read_csv('../data/test.csv')\n",
    "merchants_raw = pd.read_csv('../data/merchants.csv')\n",
    "historical_transactions_raw = pd.read_csv('../data/historical_transactions.csv')\n",
    "new_transactions_raw = pd.read_csv('../data/new_merchant_transactions.csv')\n",
    "print('...loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign to new dataframe, save the raw data to be able to reuse again\n",
    "train = train_raw\n",
    "train.name = 'train data'\n",
    "test = test_raw\n",
    "test.name = 'test data'\n",
    "historical_transactions = historical_transactions_raw\n",
    "historical_transactions.name = 'historical transaction data'\n",
    "new_transactions = new_transactions_raw\n",
    "new_transactions.name = 'new transaction data'\n",
    "merchants = merchants_raw\n",
    "merchants.name = 'merchants data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill Null Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set NA data first_active_month    0\n",
      "card_id               0\n",
      "feature_1             0\n",
      "feature_2             0\n",
      "feature_3             0\n",
      "target                0\n",
      "dtype: int64\n",
      "test set NA data first_active_month    1\n",
      "card_id               0\n",
      "feature_1             0\n",
      "feature_2             0\n",
      "feature_3             0\n",
      "dtype: int64\n",
      "new transaction set NA data authorized_flag              0\n",
      "card_id                      0\n",
      "city_id                      0\n",
      "category_1                   0\n",
      "installments                 0\n",
      "category_3               55922\n",
      "merchant_category_id         0\n",
      "merchant_id              26216\n",
      "month_lag                    0\n",
      "purchase_amount              0\n",
      "purchase_date                0\n",
      "category_2              111745\n",
      "state_id                     0\n",
      "subsector_id                 0\n",
      "dtype: int64\n",
      "historical transaction set NA data authorized_flag               0\n",
      "card_id                       0\n",
      "city_id                       0\n",
      "category_1                    0\n",
      "installments                  0\n",
      "category_3               178159\n",
      "merchant_category_id          0\n",
      "merchant_id              138481\n",
      "month_lag                     0\n",
      "purchase_amount               0\n",
      "purchase_date                 0\n",
      "category_2              2652864\n",
      "state_id                      0\n",
      "subsector_id                  0\n",
      "dtype: int64\n",
      "merchants data set NA data merchant_id                        0\n",
      "merchant_group_id                  0\n",
      "merchant_category_id               0\n",
      "subsector_id                       0\n",
      "numerical_1                        0\n",
      "numerical_2                        0\n",
      "category_1                         0\n",
      "most_recent_sales_range            0\n",
      "most_recent_purchases_range        0\n",
      "avg_sales_lag3                    13\n",
      "avg_purchases_lag3                 0\n",
      "active_months_lag3                 0\n",
      "avg_sales_lag6                    13\n",
      "avg_purchases_lag6                 0\n",
      "active_months_lag6                 0\n",
      "avg_sales_lag12                   13\n",
      "avg_purchases_lag12                0\n",
      "active_months_lag12                0\n",
      "category_4                         0\n",
      "city_id                            0\n",
      "state_id                           0\n",
      "category_2                     11887\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# view NA data\n",
    "print(\"train set NA data {}\".format(train.isna().sum()))\n",
    "print(\"test set NA data {}\".format(test.isna().sum()))\n",
    "print(\"new transaction set NA data {}\".format(new_transactions.isna().sum()))\n",
    "print(\"historical transaction set NA data {}\".format(historical_transactions.isna().sum()))\n",
    "print(\"merchants data set NA data {}\".format(merchants.isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to fill na data with random value in the same column with on-NaN data\n",
    "from random import choices\n",
    "\n",
    "def randomiseMissingData(df):\n",
    "    \"randomise missing data for DataFrame (within a column)\"\n",
    "    for col in df.columns:\n",
    "        data = df[col]\n",
    "        mask = data.isna()\n",
    "        samples = choices( data[~mask].values , k = mask.sum() )\n",
    "        data[mask] = samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fill NA data in historical transaction data\n",
      "fill NA data in new transaction data\n",
      "fill NA data in merchants data\n",
      "...done\n"
     ]
    }
   ],
   "source": [
    "# fill the NA data, this will take some time\n",
    "\n",
    "for df in [historical_transactions, new_transactions, merchants]:\n",
    "    print('fill NA data in {}'.format(df.name))\n",
    "    randomiseMissingData(df)\n",
    "print('...done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set NA data first_active_month    0\n",
      "card_id               0\n",
      "feature_1             0\n",
      "feature_2             0\n",
      "feature_3             0\n",
      "target                0\n",
      "dtype: int64\n",
      "test set NA data first_active_month    1\n",
      "card_id               0\n",
      "feature_1             0\n",
      "feature_2             0\n",
      "feature_3             0\n",
      "dtype: int64\n",
      "new transaction set NA data authorized_flag         0\n",
      "card_id                 0\n",
      "city_id                 0\n",
      "category_1              0\n",
      "installments            0\n",
      "category_3              0\n",
      "merchant_category_id    0\n",
      "merchant_id             0\n",
      "month_lag               0\n",
      "purchase_amount         0\n",
      "purchase_date           0\n",
      "category_2              0\n",
      "state_id                0\n",
      "subsector_id            0\n",
      "dtype: int64\n",
      "historical transaction set NA data authorized_flag         0\n",
      "card_id                 0\n",
      "city_id                 0\n",
      "category_1              0\n",
      "installments            0\n",
      "category_3              0\n",
      "merchant_category_id    0\n",
      "merchant_id             0\n",
      "month_lag               0\n",
      "purchase_amount         0\n",
      "purchase_date           0\n",
      "category_2              0\n",
      "state_id                0\n",
      "subsector_id            0\n",
      "dtype: int64\n",
      "merchants data set NA data merchant_id                    0\n",
      "merchant_group_id              0\n",
      "merchant_category_id           0\n",
      "subsector_id                   0\n",
      "numerical_1                    0\n",
      "numerical_2                    0\n",
      "category_1                     0\n",
      "most_recent_sales_range        0\n",
      "most_recent_purchases_range    0\n",
      "avg_sales_lag3                 0\n",
      "avg_purchases_lag3             0\n",
      "active_months_lag3             0\n",
      "avg_sales_lag6                 0\n",
      "avg_purchases_lag6             0\n",
      "active_months_lag6             0\n",
      "avg_sales_lag12                0\n",
      "avg_purchases_lag12            0\n",
      "active_months_lag12            0\n",
      "category_4                     0\n",
      "city_id                        0\n",
      "state_id                       0\n",
      "category_2                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# view NA data again\n",
    "print(\"train set NA data {}\".format(train.isna().sum()))\n",
    "print(\"test set NA data {}\".format(test.isna().sum()))\n",
    "print(\"new transaction set NA data {}\".format(new_transactions.isna().sum()))\n",
    "print(\"historical transaction set NA data {}\".format(historical_transactions.isna().sum()))\n",
    "print(\"merchants data set NA data {}\".format(merchants.isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a helper function to save memory usage, u don't need to understand this one\n",
    "# @input dataframe\n",
    "# @return dataframe\n",
    "def reduce_mem_usage(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  4.04 Mb (56.2% reduction)\n",
      "Mem. usage decreased to  2.24 Mb (52.5% reduction)\n"
     ]
    }
   ],
   "source": [
    "train = reduce_mem_usage(train)\n",
    "test = reduce_mem_usage(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "Manual feature engineering can be a tedious process (which is why we use automated feature engineering with featuretools!) and often relies on domain expertise. I will concentrate of getting as much info as possible into the final training dataframe. The idea is that the model will then pick up on which features are important rather than us having to decide that. Basically, our approach is to make as many features as possible and then give them all to the model to use! Later, we can perform feature reduction using the feature importances from the model or other techniques such as PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first_active_month', 'card_id', 'feature_1', 'feature_2', 'feature_3', 'target']\n",
      "['first_active_month', 'card_id', 'feature_1', 'feature_2', 'feature_3']\n",
      "['authorized_flag', 'card_id', 'city_id', 'category_1', 'installments', 'category_3', 'merchant_category_id', 'merchant_id', 'month_lag', 'purchase_amount', 'purchase_date', 'category_2', 'state_id', 'subsector_id']\n",
      "['authorized_flag', 'card_id', 'city_id', 'category_1', 'installments', 'category_3', 'merchant_category_id', 'merchant_id', 'month_lag', 'purchase_amount', 'purchase_date', 'category_2', 'state_id', 'subsector_id']\n",
      "['merchant_id', 'merchant_group_id', 'merchant_category_id', 'subsector_id', 'numerical_1', 'numerical_2', 'category_1', 'most_recent_sales_range', 'most_recent_purchases_range', 'avg_sales_lag3', 'avg_purchases_lag3', 'active_months_lag3', 'avg_sales_lag6', 'avg_purchases_lag6', 'active_months_lag6', 'avg_sales_lag12', 'avg_purchases_lag12', 'active_months_lag12', 'category_4', 'city_id', 'state_id', 'category_2']\n"
     ]
    }
   ],
   "source": [
    "# print all current features\n",
    "print(list(train))\n",
    "print(list(test))\n",
    "print(list(historical_transactions))\n",
    "print(list(new_transactions))\n",
    "print(list(merchants))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to understand the feature now.\n",
    "1. the train dataset, we can see that it has card_id, this should be the unique id we can use for user profile, and group or aggregate the data w.r.t this feature. the feature_# in the data_dictionary descibed as Anonymized card categorical feature, we need to understand it later.\n",
    "2. historical and new transactions. these two dataset have similar data structure.  \n",
    "    a. 'purchase_date': max','min', require feature extraction.\n",
    "        - 'month','hour','weekofyear','dayofweek','year', require no further process\n",
    "        - 'weekend': 'sum', 'mean'\n",
    "    b. 'merchant_id', 'merchant_category_id', 'subsector_id' need no more process.  \n",
    "    c. 'purchase_amount': 'sum','max','min','mean','var'.  \n",
    "    d. 'month_lag': 'max','min','mean','var'  \n",
    "    e. 'month_diff': 'mean'  \n",
    "    f. 'authorized_flag': 'sum, 'mean'  \n",
    "    g. 'category_1': 'sum', 'mean'  \n",
    "    h. 'category_2': 'mean'  \n",
    "    i. 'category_3': 'mean'  \n",
    "    j. 'card_id': 'size'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function, create new column\n",
    "def create_column(prefix, aggs):\n",
    "    return [prefix + '_' + key + '_' + agg for key in aggs.keys() for agg in aggs[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2019, 4, 29, 3, 27, 46, 577163)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data preprocessing\n",
    "\n",
    "# 1. One important feature here is purchase_date feature, we need to extract it into year, month,\n",
    "# week of year, day of week, weekend, hour\n",
    "# 2. get time difference to today which \n",
    "# 3. normalize binary data to 1/0 int\n",
    "\n",
    "\n",
    "for df in [historical_transactions, new_transactions]:\n",
    "\n",
    "    # date conversion\n",
    "    df['purchase_date'] = pd.to_datetime(df['purchase_date'])\n",
    "    df['year'] = df['purchase_date'].dt.year\n",
    "    df['weekofyear'] = df['purchase_date'].dt.weekofyear\n",
    "    df['month'] = df['purchase_date'].dt.month\n",
    "    df['dayofweek'] = df['purchase_date'].dt.dayofweek\n",
    "    df['weekend'] = (df['purchase_date'].dt.dayofweek >= 5).astype(int) # 0-5 week day\n",
    "    df['hour'] = df['purchase_date'].dt.hour\n",
    "\n",
    "    ## time difference\n",
    "    # https://www.kaggle.com/c/elo-merchant-category-recommendation/discussion/73244\n",
    "    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)//30\n",
    "    df['month_diff'] += df['month_lag']\n",
    "\n",
    "    # normalization\n",
    "    # TODO still not well done here\n",
    "    df['authorized_flag'] = df['authorized_flag'].map({'Y': 1, 'N': 0})\n",
    "    df['category_1'] = df['category_1'].map({'Y': 1, 'N': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchants['category_1'] = merchants['category_1'].map({'Y': 1, 'N': 0})\n",
    "merchants['category_4'] = merchants['category_4'].map({'Y': 1, 'N': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature aggregation\n",
    "`groupby`: group a dataframe by a column. In this case we will group by the `card_id`.  \n",
    "`agg`: perform a calculation on the grouped data such as taking the `mean` of columns. We can either call the function directly (grouped_df.mean()) or use the agg function together with a list of transforms (`grouped_df.agg([mean, max, min, sum])`)  \n",
    "`merge`: match the aggregated statistics to the appropriate client. We need to merge the original training data with the calculated stats on the `card_id` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### aggregate merchant data (group_by merchant_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_transactions_merchants(df, prefix):\n",
    "    \n",
    "    agg_func = {\n",
    "        'numerical_1': ['sum', 'mean'],\n",
    "        'numerical_2': ['sum', 'mean'],\n",
    "        'category_1': ['mean'],\n",
    "        'category_4': ['mean'],\n",
    "        'category_2': ['mean'],\n",
    "        'most_recent_sales_range_A': ['mean'],\n",
    "        'most_recent_sales_range_B': ['mean'],\n",
    "        'most_recent_sales_range_C': ['mean'],\n",
    "        'most_recent_sales_range_D': ['mean'],\n",
    "        'most_recent_sales_range_E': ['mean'],\n",
    "        'most_recent_purchases_range': ['nunique'],\n",
    "        'merchant_group_id': ['nunique'],\n",
    "        'merchant_category_id': ['nunique'],\n",
    "        'avg_sales_lag3': ['sum','mean'],\n",
    "        'avg_purchases_lag3': ['sum','mean'],\n",
    "        'active_months_lag3': ['sum', 'mean'],\n",
    "        'avg_sales_lag6': ['sum','mean'],\n",
    "        'avg_purchases_lag6': ['sum','mean'],\n",
    "        'active_months_lag6': ['sum', 'mean'],\n",
    "        'avg_sales_lag12': ['sum','mean'],\n",
    "        'avg_purchases_lag12': ['sum','mean'],\n",
    "        'active_months_lag12': ['sum', 'mean'],\n",
    "    }\n",
    "    \n",
    "    agg_df = df.groupby(['merchant_id']).agg(agg_func)\n",
    "    agg_df.columns = [prefix + '_'.join(col).strip() for col in agg_df.columns.values]\n",
    "    agg_df.reset_index(inplace=True)\n",
    "    \n",
    "    df = (df.groupby('merchant_id')\n",
    "          .size()\n",
    "          .reset_index(name='{}transactions_count'.format(prefix)))\n",
    "    \n",
    "    agg_df = pd.merge(df, agg_df, on='merchant_id', how='left')\n",
    "    \n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchants = aggregate_transactions_merchants(merchants, prefix='merch_')\n",
    "merchants = reduce_mem_usage(merchants)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge transaction data to hist transaction data, leaving this out for now since memory can't handle it\n",
    "historical_transactions = pd.merge(historical_transactions, merchants, on='merchant_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge transaction data to new transaction data\n",
    "new_transactions = pd.merge(new_transactions, merchants, on='merchant_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_transactions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### aggregate new and hist transaction data (group by card_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create aggregation columns (features)\n",
    "aggs = {}\n",
    "for col in ['month','hour','weekofyear','dayofweek','year','subsector_id','merchant_id','merchant_category_id']:\n",
    "    aggs[col] = ['nunique']\n",
    "\n",
    "aggs['purchase_amount'] = ['sum','max','min','mean','var']\n",
    "aggs['installments'] = ['sum','max','min','mean','var']\n",
    "aggs['purchase_date'] = ['max','min']\n",
    "aggs['month_lag'] = ['max','min','mean','var']\n",
    "aggs['month_diff'] = ['mean']\n",
    "aggs['authorized_flag'] = ['sum', 'mean']\n",
    "aggs['weekend'] = ['sum', 'mean']\n",
    "aggs['card_id'] = ['size']\n",
    "\n",
    "# features from merchants data\n",
    "aggs['merch_numerical_1_sum'] = ['sum', 'mean', 'max', 'min']\n",
    "aggs['merch_numerical_1_mean'] = ['sum', 'mean', 'max', 'min']\n",
    "aggs['merch_numerical_2_sum'] = ['sum', 'mean', 'max', 'min']\n",
    "aggs['merch_numerical_2_mean'] = ['sum', 'mean', 'max', 'min']\n",
    "aggs['merch_category_1_mean'] =  ['mean']\n",
    "aggs['merch_category_4_mean'] = ['mean']\n",
    "aggs['merch_category_2_mean'] = ['mean']\n",
    "aggs['merch_most_recent_sales_range_A_mean'] = ['mean']\n",
    "aggs['merch_most_recent_sales_range_B_mean'] = ['mean']\n",
    "aggs['merch_most_recent_sales_range_C_mean'] = ['mean']\n",
    "aggs['merch_most_recent_sales_range_D_mean'] = ['mean']\n",
    "aggs['merch_most_recent_sales_range_E_mean'] = ['mean']\n",
    "aggs['merch_most_recent_purchases_range_nunique'] = ['nunique']\n",
    "aggs['merch_merchant_group_id_nunique'] = ['nunique']\n",
    "aggs['merch_merchant_category_id_nunique'] = ['nunique']\n",
    "aggs['merch_avg_sales_lag3_sum'] = ['sum','mean']\n",
    "aggs['merch_avg_sales_lag3_mean'] = ['sum','mean']\n",
    "aggs['merch_avg_purchases_lag3_sum'] = ['sum','mean']\n",
    "aggs['merch_avg_purchases_lag3_mean'] = ['sum','mean']\n",
    "aggs['merch_active_months_lag3_sum'] = ['sum', 'mean']\n",
    "aggs['merch_active_months_lag3_mean'] = ['sum', 'mean']\n",
    "aggs['merch_avg_sales_lag6_sum'] = ['sum','mean']\n",
    "aggs['merch_avg_sales_lag6_mean'] = ['sum','mean']\n",
    "aggs['merch_avg_purchases_lag6_sum'] = ['sum','mean']\n",
    "aggs['merch_avg_purchases_lag6_mean'] = ['sum','mean']\n",
    "aggs['merch_active_months_lag6_sum'] = ['sum', 'mean']\n",
    "aggs['merch_active_months_lag6_mean'] = ['sum', 'mean']\n",
    "aggs['merch_avg_sales_lag12_sum'] = ['sum','mean']\n",
    "aggs['merch_avg_sales_lag12_mean'] = ['sum','mean']\n",
    "aggs['merch_avg_purchases_lag12_sum'] = ['sum','mean']\n",
    "aggs['merch_avg_purchases_lag12_mean'] = ['sum','mean']\n",
    "aggs['merch_active_months_lag12_sum'] = ['sum', 'mean']\n",
    "aggs['merch_active_months_lag12_mean'] = ['sum', 'mean']\n",
    "\n",
    "aggs['category_1'] = ['sum', 'mean']\n",
    "\n",
    "for col in ['category_2','category_3']:\n",
    "    aggs[col+'_mean'] = ['mean'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# historical transactions data feature engineering\n",
    "#\n",
    "# append a column to historical transactions first\n",
    "for col in ['category_2', 'category_3']:\n",
    "     historical_transactions[col + '_mean'] = historical_transactions.groupby([col])['purchase_amount'].transform('mean')\n",
    "\n",
    "# historical transaction features\n",
    "hist_columns = create_column('hist', aggs)\n",
    "\n",
    "hist_trans_agg = historical_transactions.groupby('card_id').agg(aggs)\n",
    "hist_trans_agg.columns = hist_columns\n",
    "hist_trans_agg.reset_index(drop=False, inplace=False)\n",
    "hist_trans_agg['hist_purchase_date_diff'] = \\\n",
    "    (hist_trans_agg['hist_purchase_date_max'] - hist_trans_agg['hist_purchase_date_min']).dt.days\n",
    "hist_trans_agg['hist_purchase_date_average'] = \\\n",
    "    (hist_trans_agg['hist_purchase_date_diff']) / hist_trans_agg['hist_card_id_size']\n",
    "hist_trans_agg['hist_purchase_date_uptonow'] = \\\n",
    "    (datetime.datetime.today() - hist_trans_agg['hist_purchase_date_max']).dt.days\n",
    "\n",
    "# merge into train and test dataset\n",
    "train = train.merge(hist_trans_agg, on='card_id', how='left')\n",
    "test = test.merge(hist_trans_agg, on='card_id', how='left')\n",
    "\n",
    "# gc collection, save memory\n",
    "del hist_trans_agg;gc.collect()\n",
    "print('...done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new transactions data feature engineering\n",
    "#\n",
    "# append a column to new transactions first\n",
    "for col in ['category_2', 'category_3']:\n",
    "     new_transactions[col + '_mean'] = new_transactions.groupby([col])['purchase_amount'].transform('mean')\n",
    "\n",
    "# new transaction features\n",
    "new_columns = create_column('new', aggs)\n",
    "\n",
    "new_trans_agg = new_transactions.groupby('card_id').agg(aggs)\n",
    "new_trans_agg.columns = new_columns\n",
    "new_trans_agg.reset_index(drop=False, inplace=False)\n",
    "new_trans_agg['new_purchase_date_diff'] = \\\n",
    "    (new_trans_agg['new_purchase_date_max'] - new_trans_agg['new_purchase_date_min']).dt.days\n",
    "new_trans_agg['new_purchase_date_average'] = \\\n",
    "    (new_trans_agg['new_purchase_date_diff']) / new_trans_agg['new_card_id_size']\n",
    "new_trans_agg['new_purchase_date_uptonow'] = \\\n",
    "    (datetime.datetime.today() - new_trans_agg['new_purchase_date_max']).dt.days\n",
    "\n",
    "# merge into train and test dataset\n",
    "train = train.merge(new_trans_agg, on='card_id', how='left')\n",
    "test = test.merge(new_trans_agg, on='card_id', how='left')\n",
    "\n",
    "# gc collection\n",
    "del new_trans_agg;gc.collect()\n",
    "print('...done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gc collection\n",
    "del historical_transactions;gc.collect()\n",
    "del new_transactions;gc.collect()\n",
    "print('...done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outliers\n",
    "train['outliers'] = 0\n",
    "train.loc[train['target'] < -30, 'outliers'] = 1\n",
    "train['outliers'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test dataset feature engineering\n",
    "for df in [train, test]:\n",
    "    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
    "    df['dayofweek'] = df['first_active_month'].dt.dayofweek\n",
    "    df['weekofyear'] = df['first_active_month'].dt.weekofyear\n",
    "    df['month'] = df['first_active_month'].dt.month\n",
    "    df['elapsed_time'] = (datetime.datetime.today() - df['first_active_month']).dt.days\n",
    "    df['hist_first_buy'] = (df['hist_purchase_date_min'] - df['first_active_month']).dt.days\n",
    "    df['new_first_buy'] = (df['new_purchase_date_min'] - df['first_active_month']).dt.days\n",
    "    for f in ['hist_purchase_date_max','hist_purchase_date_min','new_purchase_date_max',\\\n",
    "                     'new_purchase_date_min']:\n",
    "        df[f] = df[f].astype(np.int64) * 1e-9\n",
    "    df['card_id_total'] = df['hist_card_id_size'] + df['new_card_id_size']\n",
    "    df['purchase_amount_total'] = df['new_purchase_amount_sum'] + df['hist_purchase_amount_sum']\n",
    "\n",
    "for f in ['feature_1','feature_2','feature_3']:\n",
    "    order_label = train.groupby([f])['outliers'].mean()\n",
    "    train[f] = train[f].map(order_label)\n",
    "    test[f] = test[f].map(order_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.3)\n",
    "plt.subplot(231)\n",
    "plt.hist(train['feature_1'])\n",
    "plt.xlabel('feature_1')\n",
    "plt.ylabel('frequency')\n",
    "plt.subplot(232)\n",
    "plt.hist(train['feature_2'])\n",
    "plt.xlabel('feature_2')\n",
    "plt.ylabel('frequency')\n",
    "plt.subplot(233)\n",
    "plt.hist(train['feature_3'])\n",
    "plt.xlabel('feature_3')\n",
    "plt.ylabel('frequency')\n",
    "plt.subplot(234)\n",
    "plt.hist(train['target'])\n",
    "plt.xlabel('target')\n",
    "plt.ylabel('frequency')\n",
    "plt.subplot(235)\n",
    "ax = sns.lineplot(x = \"month\", y = \"target\", \n",
    "                  markers = True, dashes = False, data = train)\n",
    "plt.xticks(rotation = 45)\n",
    "ax.set_xlabel('Purchase Month')\n",
    "plt.subplot(236)\n",
    "ax = sns.lineplot(x = \"weekofyear\", y = \"target\", \n",
    "                  markers = True, dashes = False, data = train)\n",
    "plt.xticks(rotation = 45)\n",
    "ax.set_xlabel('week of year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the modeling now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn import neighbors\n",
    "\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_columns = [c for c in train.columns if c not in ['card_id', 'first_active_month','target','outliers']]\n",
    "target = train['target']\n",
    "del train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try some models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightgbm(train, test, n_folds=5):\n",
    "    param = {'num_leaves': 31,\n",
    "         'min_data_in_leaf': 30, \n",
    "         'objective':'regression',\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.01,\n",
    "         \"min_child_samples\": 20,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"verbosity\": -1,\n",
    "         \"nthread\": 4,\n",
    "         \"random_state\": 4590}\n",
    "    folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=4590)\n",
    "    oof = np.zeros(len(train))\n",
    "    predictions = np.zeros(len(test))\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['outliers'].values)):\n",
    "        print(\"fold {}\".format(fold_))\n",
    "        trn_data = lgb.Dataset(train.iloc[trn_idx][train_columns], label=target.iloc[trn_idx])#, categorical_feature=categorical_feats)\n",
    "        val_data = lgb.Dataset(train.iloc[val_idx][train_columns], label=target.iloc[val_idx])#, categorical_feature=categorical_feats)\n",
    "\n",
    "        num_round = 10000\n",
    "        clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 100)\n",
    "        oof[val_idx] = clf.predict(train.iloc[val_idx][train_columns], num_iteration=clf.best_iteration)\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"Feature\"] = train_columns\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "        fold_importance_df[\"fold\"] = fold_ + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "\n",
    "        predictions += clf.predict(test[train_columns], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "    valid_score = np.sqrt(mean_squared_error(oof, target))\n",
    "    return feature_importance_df, valid_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance, valid_score = lightgbm(train, test, n_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"the best validation RMSE score is {}\".format(valid_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the features\n",
    "sns.set(font_scale=1.5)\n",
    "cols = (feature_importance[[\"Feature\", \"importance\"]]\n",
    "        .groupby(\"Feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:20].index)\n",
    "\n",
    "best_features = feature_importance.loc[feature_importance.Feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(10,15))\n",
    "sns.barplot(x=\"importance\",\n",
    "            y=\"Feature\",\n",
    "            data=best_features.sort_values(by=\"importance\",\n",
    "                                           ascending=False))\n",
    "plt.title('LightGBM Top 20 Features (avg over folds)')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('lgbm_importances.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elo-cee",
   "language": "python",
   "name": "elo-cee"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
