{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final\n",
    "Feature Engineering is the key.\n",
    "\n",
    "  \n",
    "reference:  \n",
    "https://www.kaggle.com/mjbahmani/statistical-analysis-for-elo  \n",
    "https://www.kaggle.com/c/elo-merchant-category-recommendation/discussion/82055  \n",
    "https://www.kaggle.com/chauhuynh/my-first-kernel-3-699  \n",
    "https://www.kaggle.com/fabiendaniel/elo-world  \n",
    "https://www.kaggle.com/raddar/target-true-meaning-revealed  \n",
    "https://www.kaggle.com/c/elo-merchant-category-recommendation/discussion/82036#479038  \n",
    "https://machinelearningmastery.com/basic-feature-engineering-time-series-data-python/  \n",
    "https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering this is really helpfull  \n",
    "https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc\n",
    "\n",
    "lightgbm params tuning:\n",
    "https://www.kaggle.com/garethjns/microsoft-lightgbm-with-parameter-tuning-0-823\n",
    "\n",
    "datetime feature  \n",
    "https://datascience.stackexchange.com/questions/27273/does-it-make-sense-that-datetime-encodes-one-hot-vector-like-one-hot-encoding-or  \n",
    "parameter tuning: https://www.kaggle.com/fabiendaniel/hyperparameter-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan\n",
    "After the analysis of mid-term project, we achieved a performance (RMSE) at 3.6524 on validation dataset using lightGBM model. Now we need to think can we do better on it? So we are planning to conduct the analysis further:\n",
    "1. Can we get better score by using different or more features?\n",
    "2. Can we get better score by optimizing the model?\n",
    "3. Can we get better score by dealing with outliers more properly?\n",
    "4. we need to do CV.\n",
    "5. Xgboost/Catboost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data_Dictionary.xlsx', 'new_merchant_transactions.csv', 'test.csv', 'merchants.csv', 'historical_transactions.csv', 'train.csv', '.ipynb_checkpoints', 'sample_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(4590)\n",
    "\n",
    "print(os.listdir(\"../data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...loaded\n"
     ]
    }
   ],
   "source": [
    "train_raw = pd.read_csv('../data/train.csv')\n",
    "test_raw = pd.read_csv('../data/test.csv')\n",
    "merchants_raw = pd.read_csv('../data/merchants.csv')\n",
    "historical_transactions_raw = pd.read_csv('../data/historical_transactions.csv')\n",
    "new_transactions_raw = pd.read_csv('../data/new_merchant_transactions.csv')\n",
    "print('...loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign to new dataframe, save the raw data to be able to reuse again\n",
    "train = train_raw\n",
    "train.name = 'train data'\n",
    "test = test_raw\n",
    "test.name = 'test data'\n",
    "historical_transactions = historical_transactions_raw\n",
    "historical_transactions.name = 'historical transaction data'\n",
    "new_transactions = new_transactions_raw\n",
    "new_transactions.name = 'new transaction data'\n",
    "merchants = merchants_raw\n",
    "merchants.name = 'merchants data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill Null Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set NA data first_active_month    0\n",
      "card_id               0\n",
      "feature_1             0\n",
      "feature_2             0\n",
      "feature_3             0\n",
      "target                0\n",
      "dtype: int64\n",
      "test set NA data first_active_month    1\n",
      "card_id               0\n",
      "feature_1             0\n",
      "feature_2             0\n",
      "feature_3             0\n",
      "dtype: int64\n",
      "new transaction set NA data authorized_flag              0\n",
      "card_id                      0\n",
      "city_id                      0\n",
      "category_1                   0\n",
      "installments                 0\n",
      "category_3               55922\n",
      "merchant_category_id         0\n",
      "merchant_id              26216\n",
      "month_lag                    0\n",
      "purchase_amount              0\n",
      "purchase_date                0\n",
      "category_2              111745\n",
      "state_id                     0\n",
      "subsector_id                 0\n",
      "dtype: int64\n",
      "historical transaction set NA data authorized_flag               0\n",
      "card_id                       0\n",
      "city_id                       0\n",
      "category_1                    0\n",
      "installments                  0\n",
      "category_3               178159\n",
      "merchant_category_id          0\n",
      "merchant_id              138481\n",
      "month_lag                     0\n",
      "purchase_amount               0\n",
      "purchase_date                 0\n",
      "category_2              2652864\n",
      "state_id                      0\n",
      "subsector_id                  0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# view NA data\n",
    "print(\"train set NA data {}\".format(train.isna().sum()))\n",
    "print(\"test set NA data {}\".format(test.isna().sum()))\n",
    "print(\"new transaction set NA data {}\".format(new_transactions.isna().sum()))\n",
    "print(\"historical transaction set NA data {}\".format(historical_transactions.isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to fill na data with random value in the same column with on-NaN data\n",
    "from random import choices\n",
    "\n",
    "def randomiseMissingData(df):\n",
    "    \"randomise missing data for DataFrame (within a column)\"\n",
    "    for col in df.columns:\n",
    "        data = df[col]\n",
    "        mask = data.isna()\n",
    "        samples = choices( data[~mask].values , k = mask.sum() )\n",
    "        data[mask] = samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fill NA data in historical transaction data\n",
      "fill NA data in new transaction data\n",
      "...done\n"
     ]
    }
   ],
   "source": [
    "# fill the NA data, this will take some time\n",
    "\n",
    "for df in [historical_transactions, new_transactions]:\n",
    "    print('fill NA data in {}'.format(df.name))\n",
    "    randomiseMissingData(df)\n",
    "print('...done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set NA data first_active_month    0\n",
      "card_id               0\n",
      "feature_1             0\n",
      "feature_2             0\n",
      "feature_3             0\n",
      "target                0\n",
      "dtype: int64\n",
      "test set NA data first_active_month    1\n",
      "card_id               0\n",
      "feature_1             0\n",
      "feature_2             0\n",
      "feature_3             0\n",
      "dtype: int64\n",
      "new transaction set NA data authorized_flag         0\n",
      "card_id                 0\n",
      "city_id                 0\n",
      "category_1              0\n",
      "installments            0\n",
      "category_3              0\n",
      "merchant_category_id    0\n",
      "merchant_id             0\n",
      "month_lag               0\n",
      "purchase_amount         0\n",
      "purchase_date           0\n",
      "category_2              0\n",
      "state_id                0\n",
      "subsector_id            0\n",
      "dtype: int64\n",
      "historical transaction set NA data authorized_flag         0\n",
      "card_id                 0\n",
      "city_id                 0\n",
      "category_1              0\n",
      "installments            0\n",
      "category_3              0\n",
      "merchant_category_id    0\n",
      "merchant_id             0\n",
      "month_lag               0\n",
      "purchase_amount         0\n",
      "purchase_date           0\n",
      "category_2              0\n",
      "state_id                0\n",
      "subsector_id            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# view NA data again\n",
    "print(\"train set NA data {}\".format(train.isna().sum()))\n",
    "print(\"test set NA data {}\".format(test.isna().sum()))\n",
    "print(\"new transaction set NA data {}\".format(new_transactions.isna().sum()))\n",
    "print(\"historical transaction set NA data {}\".format(historical_transactions.isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a helper function to save memory usage, u don't need to understand this one\n",
    "# @input dataframe\n",
    "# @return dataframe\n",
    "def reduce_mem_usage(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  4.04 Mb (56.2% reduction)\n",
      "Mem. usage decreased to  2.24 Mb (52.5% reduction)\n"
     ]
    }
   ],
   "source": [
    "train = reduce_mem_usage(train)\n",
    "test = reduce_mem_usage(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "Manual feature engineering can be a tedious process (which is why we use automated feature engineering with featuretools!) and often relies on domain expertise. I will concentrate of getting as much info as possible into the final training dataframe. The idea is that the model will then pick up on which features are important rather than us having to decide that. Basically, our approach is to make as many features as possible and then give them all to the model to use! Later, we can perform feature reduction using the feature importances from the model or other techniques such as PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first_active_month', 'card_id', 'feature_1', 'feature_2', 'feature_3', 'target']\n",
      "['first_active_month', 'card_id', 'feature_1', 'feature_2', 'feature_3']\n",
      "['authorized_flag', 'card_id', 'city_id', 'category_1', 'installments', 'category_3', 'merchant_category_id', 'merchant_id', 'month_lag', 'purchase_amount', 'purchase_date', 'category_2', 'state_id', 'subsector_id']\n",
      "['authorized_flag', 'card_id', 'city_id', 'category_1', 'installments', 'category_3', 'merchant_category_id', 'merchant_id', 'month_lag', 'purchase_amount', 'purchase_date', 'category_2', 'state_id', 'subsector_id']\n"
     ]
    }
   ],
   "source": [
    "# print all current features\n",
    "print(list(train))\n",
    "print(list(test))\n",
    "print(list(historical_transactions))\n",
    "print(list(new_transactions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to understand the feature now.\n",
    "1. the train dataset, we can see that it has card_id, this should be the unique id we can use for user profile, and group or aggregate the data w.r.t this feature. the feature_# in the data_dictionary descibed as Anonymized card categorical feature, we need to understand it later.\n",
    "2. historical and new transactions. these two dataset have similar data structure.  \n",
    "    a. 'purchase_date': max','min', require feature extraction.\n",
    "        - 'month','hour','weekofyear','dayofweek','year', require no further process\n",
    "        - 'weekend': 'sum', 'mean'\n",
    "    b. 'merchant_id', 'merchant_category_id', 'subsector_id' need no more process.  \n",
    "    c. 'purchase_amount': 'sum','max','min','mean','var'.  \n",
    "    d. 'month_lag': 'max','min','mean','var'  \n",
    "    e. 'month_diff': 'mean'  \n",
    "    f. 'authorized_flag': 'sum, 'mean'  \n",
    "    g. 'category_1': 'sum', 'mean'  \n",
    "    h. 'category_2': 'mean'  \n",
    "    i. 'category_3': 'mean'  \n",
    "    j. 'card_id': 'size'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function, create new column\n",
    "def create_column(prefix, aggs):\n",
    "    return [prefix + '_' + key + '_' + agg for key in aggs.keys() for agg in aggs[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2019, 4, 30, 17, 38, 24, 396550)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## main data preprocessing block\n",
    "\n",
    "# 1. One important feature here is purchase_date feature, we need to extract it into year, month,\n",
    "# week of year, day of week, weekend, hour\n",
    "# 2. get time difference to today which \n",
    "# 3. normalize binary data to 1/0 int\n",
    "\n",
    "\n",
    "for df in [historical_transactions, new_transactions]:\n",
    "\n",
    "    # date conversion\n",
    "    df['purchase_date'] = pd.to_datetime(df['purchase_date'])\n",
    "    df['year'] = df['purchase_date'].dt.year\n",
    "    df['weekofyear'] = df['purchase_date'].dt.weekofyear\n",
    "    df['month'] = df['purchase_date'].dt.month\n",
    "    df['dayofweek'] = df['purchase_date'].dt.dayofweek\n",
    "    df['weekend'] = (df['purchase_date'].dt.dayofweek >= 5).astype(int) # 0-5 week day\n",
    "    df['hour'] = df['purchase_date'].dt.hour\n",
    "\n",
    "    ## time difference\n",
    "    # https://www.kaggle.com/c/elo-merchant-category-recommendation/discussion/73244\n",
    "    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)//30\n",
    "    df['month_diff'] += df['month_lag']\n",
    "\n",
    "    # normalization\n",
    "    # TODO still not well done here\n",
    "    df['authorized_flag'] = df['authorized_flag'].map({'Y': 1, 'N': 0})\n",
    "    df['category_1'] = df['category_1'].map({'Y': 1, 'N': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature aggregation\n",
    "`groupby`: group a dataframe by a column. In this case we will group by the `card_id`.  \n",
    "`agg`: perform a calculation on the grouped data such as taking the `mean` of columns. We can either call the function directly (grouped_df.mean()) or use the agg function together with a list of transforms (`grouped_df.agg([mean, max, min, sum])`)  \n",
    "`merge`: match the aggregated statistics to the appropriate client. We need to merge the original training data with the calculated stats on the `card_id` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create aggregation columns (features)\n",
    "aggs = {}\n",
    "for col in ['month','hour','weekofyear','dayofweek','year','subsector_id','merchant_id','merchant_category_id']:\n",
    "    aggs[col] = ['nunique']\n",
    "\n",
    "aggs['purchase_amount'] = ['sum','max','min','mean','var']\n",
    "aggs['installments'] = ['sum','max','min','mean','var']\n",
    "aggs['purchase_date'] = ['max','min']\n",
    "aggs['month_lag'] = ['max','min','mean','var']\n",
    "aggs['month_diff'] = ['mean']\n",
    "aggs['authorized_flag'] = ['sum', 'mean']\n",
    "aggs['weekend'] = ['sum', 'mean']\n",
    "aggs['card_id'] = ['size']\n",
    "\n",
    "aggs['category_1'] = ['sum', 'mean']\n",
    "\n",
    "for col in ['category_2','category_3']:\n",
    "    aggs[col+'_mean'] = ['mean'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'month': ['nunique'],\n",
       " 'hour': ['nunique'],\n",
       " 'weekofyear': ['nunique'],\n",
       " 'dayofweek': ['nunique'],\n",
       " 'year': ['nunique'],\n",
       " 'subsector_id': ['nunique'],\n",
       " 'merchant_id': ['nunique'],\n",
       " 'merchant_category_id': ['nunique'],\n",
       " 'purchase_amount': ['sum', 'max', 'min', 'mean', 'var'],\n",
       " 'installments': ['sum', 'max', 'min', 'mean', 'var'],\n",
       " 'purchase_date': ['max', 'min'],\n",
       " 'month_lag': ['max', 'min', 'mean', 'var'],\n",
       " 'month_diff': ['mean'],\n",
       " 'authorized_flag': ['sum', 'mean'],\n",
       " 'weekend': ['sum', 'mean'],\n",
       " 'card_id': ['size'],\n",
       " 'category_1': ['sum', 'mean'],\n",
       " 'category_2_mean': ['mean'],\n",
       " 'category_3_mean': ['mean']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...done\n"
     ]
    }
   ],
   "source": [
    "# historical transactions data feature engineering\n",
    "#\n",
    "# append a column to historical transactions first\n",
    "for col in ['category_2', 'category_3']:\n",
    "     historical_transactions[col + '_mean'] = \\\n",
    "        historical_transactions.groupby([col])['purchase_amount'].transform('mean')\n",
    "\n",
    "# historical transaction features\n",
    "hist_columns = create_column('hist', aggs)\n",
    "\n",
    "hist_trans_agg = historical_transactions.groupby('card_id').agg(aggs)\n",
    "hist_trans_agg.columns = hist_columns\n",
    "hist_trans_agg.reset_index(drop=False, inplace=False)\n",
    "hist_trans_agg['hist_purchase_date_diff'] = \\\n",
    "    (hist_trans_agg['hist_purchase_date_max'] - hist_trans_agg['hist_purchase_date_min']).dt.days\n",
    "hist_trans_agg['hist_purchase_date_average'] = \\\n",
    "    (hist_trans_agg['hist_purchase_date_diff']) / hist_trans_agg['hist_card_id_size']\n",
    "hist_trans_agg['hist_purchase_date_uptonow'] = \\\n",
    "    (datetime.datetime.today() - hist_trans_agg['hist_purchase_date_max']).dt.days\n",
    "\n",
    "# merge into train and test dataset\n",
    "train = train.merge(hist_trans_agg, on='card_id', how='left')\n",
    "test = test.merge(hist_trans_agg, on='card_id', how='left')\n",
    "\n",
    "# gc collection, save memory\n",
    "del hist_trans_agg;gc.collect()\n",
    "print('...done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...done\n"
     ]
    }
   ],
   "source": [
    "# new transactions data feature engineering\n",
    "#\n",
    "# append a column to new transactions first\n",
    "for col in ['category_2', 'category_3']:\n",
    "     new_transactions[col + '_mean'] = \\\n",
    "        new_transactions.groupby([col])['purchase_amount'].transform('mean')\n",
    "\n",
    "# new transaction features\n",
    "new_columns = create_column('new', aggs)\n",
    "\n",
    "new_trans_agg = new_transactions.groupby('card_id').agg(aggs)\n",
    "new_trans_agg.columns = new_columns\n",
    "new_trans_agg.reset_index(drop=False, inplace=False)\n",
    "new_trans_agg['new_purchase_date_diff'] = \\\n",
    "    (new_trans_agg['new_purchase_date_max'] - new_trans_agg['new_purchase_date_min']).dt.days\n",
    "new_trans_agg['new_purchase_date_average'] = \\\n",
    "    (new_trans_agg['new_purchase_date_diff']) / new_trans_agg['new_card_id_size']\n",
    "new_trans_agg['new_purchase_date_uptonow'] = \\\n",
    "    (datetime.datetime.today() - new_trans_agg['new_purchase_date_max']).dt.days\n",
    "\n",
    "# merge into train and test dataset\n",
    "train = train.merge(new_trans_agg, on='card_id', how='left')\n",
    "test = test.merge(new_trans_agg, on='card_id', how='left')\n",
    "\n",
    "# gc collection\n",
    "del new_trans_agg;gc.collect()\n",
    "print('...done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...done\n"
     ]
    }
   ],
   "source": [
    "# gc collection\n",
    "del historical_transactions;gc.collect()\n",
    "del new_transactions;gc.collect()\n",
    "print('...done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    199710\n",
       "1      2207\n",
       "Name: outliers, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# outliers\n",
    "train['outliers'] = 0\n",
    "train.loc[train['target'] < -30, 'outliers'] = 1\n",
    "train['outliers'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test dataset feature engineering\n",
    "for df in [train, test]:\n",
    "    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
    "    df['dayofweek'] = df['first_active_month'].dt.dayofweek\n",
    "    df['weekofyear'] = df['first_active_month'].dt.weekofyear\n",
    "    df['month'] = df['first_active_month'].dt.month\n",
    "    df['elapsed_time'] = (datetime.datetime.today() - df['first_active_month']).dt.days\n",
    "    df['hist_first_buy'] = (df['hist_purchase_date_min'] - df['first_active_month']).dt.days\n",
    "    df['new_first_buy'] = (df['new_purchase_date_min'] - df['first_active_month']).dt.days\n",
    "    for f in ['hist_purchase_date_max','hist_purchase_date_min','new_purchase_date_max',\\\n",
    "                     'new_purchase_date_min']:\n",
    "        df[f] = df[f].astype(np.int64) * 1e-9\n",
    "    df['card_id_total'] = df['hist_card_id_size'] + df['new_card_id_size']\n",
    "    df['purchase_amount_total'] = df['new_purchase_amount_sum'] + df['hist_purchase_amount_sum']\n",
    "\n",
    "for f in ['feature_1','feature_2','feature_3']:\n",
    "    order_label = train.groupby([f])['outliers'].mean()\n",
    "    train[f] = train[f].map(order_label)\n",
    "    test[f] = test[f].map(order_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_active_month</th>\n",
       "      <th>card_id</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>target</th>\n",
       "      <th>hist_month_nunique</th>\n",
       "      <th>hist_hour_nunique</th>\n",
       "      <th>hist_weekofyear_nunique</th>\n",
       "      <th>hist_dayofweek_nunique</th>\n",
       "      <th>...</th>\n",
       "      <th>new_purchase_date_uptonow</th>\n",
       "      <th>outliers</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>month</th>\n",
       "      <th>elapsed_time</th>\n",
       "      <th>hist_first_buy</th>\n",
       "      <th>new_first_buy</th>\n",
       "      <th>card_id_total</th>\n",
       "      <th>purchase_amount_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>C_ID_92a2005557</td>\n",
       "      <td>0.013145</td>\n",
       "      <td>0.008752</td>\n",
       "      <td>0.011428</td>\n",
       "      <td>-0.820312</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>366.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>698</td>\n",
       "      <td>26</td>\n",
       "      <td>277.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>-179.212942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>C_ID_3d0044924f</td>\n",
       "      <td>0.010712</td>\n",
       "      <td>0.011385</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>0.392822</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>396.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>849</td>\n",
       "      <td>5</td>\n",
       "      <td>396.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>-214.362071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-08-01</td>\n",
       "      <td>C_ID_d639edf6cd</td>\n",
       "      <td>0.010610</td>\n",
       "      <td>0.008752</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>0.687988</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>366.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>8</td>\n",
       "      <td>1002</td>\n",
       "      <td>163</td>\n",
       "      <td>635.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>-29.867717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>C_ID_186d6a6901</td>\n",
       "      <td>0.010712</td>\n",
       "      <td>0.014166</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>0.142456</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>377.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>9</td>\n",
       "      <td>606</td>\n",
       "      <td>25</td>\n",
       "      <td>187.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>-54.145736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>C_ID_cdbd2c0db2</td>\n",
       "      <td>0.008058</td>\n",
       "      <td>0.014166</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>-0.159790</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>366.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "      <td>11</td>\n",
       "      <td>545</td>\n",
       "      <td>11</td>\n",
       "      <td>121.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>-68.613893</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_active_month          card_id  feature_1  feature_2  feature_3  \\\n",
       "0         2017-06-01  C_ID_92a2005557   0.013145   0.008752   0.011428   \n",
       "1         2017-01-01  C_ID_3d0044924f   0.010712   0.011385   0.010283   \n",
       "2         2016-08-01  C_ID_d639edf6cd   0.010610   0.008752   0.010283   \n",
       "3         2017-09-01  C_ID_186d6a6901   0.010712   0.014166   0.010283   \n",
       "4         2017-11-01  C_ID_cdbd2c0db2   0.008058   0.014166   0.010283   \n",
       "\n",
       "     target  hist_month_nunique  hist_hour_nunique  hist_weekofyear_nunique  \\\n",
       "0 -0.820312                   9                 23                       35   \n",
       "1  0.392822                  12                 24                       50   \n",
       "2  0.687988                  10                 14                       22   \n",
       "3  0.142456                   6                 16                       20   \n",
       "4 -0.159790                   4                 22                       17   \n",
       "\n",
       "   hist_dayofweek_nunique  ...  new_purchase_date_uptonow  outliers  \\\n",
       "0                       7  ...                      366.0         0   \n",
       "1                       7  ...                      396.0         0   \n",
       "2                       7  ...                      366.0         0   \n",
       "3                       7  ...                      377.0         0   \n",
       "4                       7  ...                      366.0         0   \n",
       "\n",
       "   dayofweek  weekofyear  month  elapsed_time  hist_first_buy  new_first_buy  \\\n",
       "0          3          22      6           698              26          277.0   \n",
       "1          6          52      1           849               5          396.0   \n",
       "2          0          31      8          1002             163          635.0   \n",
       "3          4          35      9           606              25          187.0   \n",
       "4          2          44     11           545              11          121.0   \n",
       "\n",
       "   card_id_total  purchase_amount_total  \n",
       "0          283.0            -179.212942  \n",
       "1          356.0            -214.362071  \n",
       "2           44.0             -29.867717  \n",
       "3           84.0             -54.145736  \n",
       "4          169.0             -68.613893  \n",
       "\n",
       "[5 rows x 89 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the modeling now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn import neighbors\n",
    "\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_columns = [c for c in train.columns if c not in ['card_id', 'first_active_month','target','outliers']]\n",
    "target = train['target']\n",
    "del train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_active_month</th>\n",
       "      <th>card_id</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>hist_month_nunique</th>\n",
       "      <th>hist_hour_nunique</th>\n",
       "      <th>hist_weekofyear_nunique</th>\n",
       "      <th>hist_dayofweek_nunique</th>\n",
       "      <th>hist_year_nunique</th>\n",
       "      <th>...</th>\n",
       "      <th>new_purchase_date_uptonow</th>\n",
       "      <th>outliers</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>month</th>\n",
       "      <th>elapsed_time</th>\n",
       "      <th>hist_first_buy</th>\n",
       "      <th>new_first_buy</th>\n",
       "      <th>card_id_total</th>\n",
       "      <th>purchase_amount_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>C_ID_92a2005557</td>\n",
       "      <td>0.013145</td>\n",
       "      <td>0.008752</td>\n",
       "      <td>0.011428</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>366.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>698</td>\n",
       "      <td>26</td>\n",
       "      <td>277.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>-179.212942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>C_ID_3d0044924f</td>\n",
       "      <td>0.010712</td>\n",
       "      <td>0.011385</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>396.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>849</td>\n",
       "      <td>5</td>\n",
       "      <td>396.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>-214.362071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-08-01</td>\n",
       "      <td>C_ID_d639edf6cd</td>\n",
       "      <td>0.010610</td>\n",
       "      <td>0.008752</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>366.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>8</td>\n",
       "      <td>1002</td>\n",
       "      <td>163</td>\n",
       "      <td>635.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>-29.867717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>C_ID_186d6a6901</td>\n",
       "      <td>0.010712</td>\n",
       "      <td>0.014166</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>377.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>9</td>\n",
       "      <td>606</td>\n",
       "      <td>25</td>\n",
       "      <td>187.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>-54.145736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>C_ID_cdbd2c0db2</td>\n",
       "      <td>0.008058</td>\n",
       "      <td>0.014166</td>\n",
       "      <td>0.010283</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>366.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "      <td>11</td>\n",
       "      <td>545</td>\n",
       "      <td>11</td>\n",
       "      <td>121.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>-68.613893</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_active_month          card_id  feature_1  feature_2  feature_3  \\\n",
       "0         2017-06-01  C_ID_92a2005557   0.013145   0.008752   0.011428   \n",
       "1         2017-01-01  C_ID_3d0044924f   0.010712   0.011385   0.010283   \n",
       "2         2016-08-01  C_ID_d639edf6cd   0.010610   0.008752   0.010283   \n",
       "3         2017-09-01  C_ID_186d6a6901   0.010712   0.014166   0.010283   \n",
       "4         2017-11-01  C_ID_cdbd2c0db2   0.008058   0.014166   0.010283   \n",
       "\n",
       "   hist_month_nunique  hist_hour_nunique  hist_weekofyear_nunique  \\\n",
       "0                   9                 23                       35   \n",
       "1                  12                 24                       50   \n",
       "2                  10                 14                       22   \n",
       "3                   6                 16                       20   \n",
       "4                   4                 22                       17   \n",
       "\n",
       "   hist_dayofweek_nunique  hist_year_nunique  ...  new_purchase_date_uptonow  \\\n",
       "0                       7                  2  ...                      366.0   \n",
       "1                       7                  2  ...                      396.0   \n",
       "2                       7                  2  ...                      366.0   \n",
       "3                       7                  2  ...                      377.0   \n",
       "4                       7                  2  ...                      366.0   \n",
       "\n",
       "   outliers  dayofweek  weekofyear  month  elapsed_time  hist_first_buy  \\\n",
       "0         0          3          22      6           698              26   \n",
       "1         0          6          52      1           849               5   \n",
       "2         0          0          31      8          1002             163   \n",
       "3         0          4          35      9           606              25   \n",
       "4         0          2          44     11           545              11   \n",
       "\n",
       "   new_first_buy  card_id_total  purchase_amount_total  \n",
       "0          277.0          283.0            -179.212942  \n",
       "1          396.0          356.0            -214.362071  \n",
       "2          635.0           44.0             -29.867717  \n",
       "3          187.0           84.0             -54.145736  \n",
       "4          121.0          169.0             -68.613893  \n",
       "\n",
       "[5 rows x 88 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try some models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.8203  0.3928  0.688  ...  0.0935 -4.676  -1.859 ]\n"
     ]
    }
   ],
   "source": [
    "print(target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightgbm(train, test, n_folds=5):\n",
    "    param = {'num_leaves': 111,\n",
    "         'min_data_in_leaf': 149, \n",
    "         'objective':'regression',\n",
    "         'max_depth': 9,\n",
    "         'learning_rate': 0.005,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.7522,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.7083 ,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.2634,\n",
    "         \"random_state\": 133,\n",
    "         \"verbosity\": -1}\n",
    "    folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=4590)\n",
    "    oof = np.zeros(len(train))\n",
    "    predictions = np.zeros(len(test))\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['outliers'].values)):\n",
    "        print(\"fold {}\".format(fold_))\n",
    "        trn_data = lgb.Dataset(train.iloc[trn_idx][train_columns], label=target.iloc[trn_idx])#, categorical_feature=categorical_feats)\n",
    "        val_data = lgb.Dataset(train.iloc[val_idx][train_columns], label=target.iloc[val_idx])#, categorical_feature=categorical_feats)\n",
    "\n",
    "        num_round = 10000\n",
    "        clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 100)\n",
    "        oof[val_idx] = clf.predict(train.iloc[val_idx][train_columns], num_iteration=clf.best_iteration)\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"Feature\"] = train_columns\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "        fold_importance_df[\"fold\"] = fold_ + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "\n",
    "        predictions += clf.predict(test[train_columns], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "    valid_score = np.sqrt(mean_squared_error(oof, target))\n",
    "    return feature_importance_df, valid_score, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_importance, valid_score, predictions = lightgbm(train, test, n_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_feature_important():\n",
    "    # plot the features\n",
    "    sns.set(font_scale=1.5)\n",
    "    cols = (feature_importance[[\"Feature\", \"importance\"]]\n",
    "            .groupby(\"Feature\")\n",
    "            .mean()\n",
    "            .sort_values(by=\"importance\", ascending=False)[:20].index)\n",
    "\n",
    "    best_features = feature_importance.loc[feature_importance.Feature.isin(cols)]\n",
    "\n",
    "    plt.figure(figsize=(10,15))\n",
    "    sns.barplot(x=\"importance\",\n",
    "                y=\"Feature\",\n",
    "                data=best_features.sort_values(by=\"importance\",\n",
    "                                               ascending=False))\n",
    "    plt.title('LightGBM Top 20 Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('lgbm_importances.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [c for c in train.columns if c not in ['card_id', 'first_active_month']]\n",
    "categorical_feats = [c for c in features if 'feature_' in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM CV with parameter tuning\n",
    "# use train and target data\n",
    "def LGB_CV(\n",
    "          max_depth,\n",
    "          num_leaves,\n",
    "          min_data_in_leaf,\n",
    "          feature_fraction,\n",
    "          bagging_fraction,\n",
    "          lambda_l1\n",
    "         ):\n",
    "    \n",
    "    folds = KFold(n_splits=5, shuffle=True, random_state=15)\n",
    "    oof = np.zeros(train.shape[0])\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n",
    "        print(\"fold n°{}\".format(fold_))\n",
    "        trn_data = lgb.Dataset(train.iloc[trn_idx][features],\n",
    "                               label=target.iloc[trn_idx],\n",
    "                               categorical_feature=categorical_feats)\n",
    "        val_data = lgb.Dataset(train.iloc[val_idx][features],\n",
    "                               label=target.iloc[val_idx],\n",
    "                               categorical_feature=categorical_feats)\n",
    "    \n",
    "        param = {\n",
    "            'num_leaves': int(num_leaves),\n",
    "            'min_data_in_leaf': int(min_data_in_leaf), \n",
    "            'objective':'regression',\n",
    "            'max_depth': int(max_depth),\n",
    "            'learning_rate': 0.01,\n",
    "            \"boosting\": \"gbdt\",\n",
    "            \"feature_fraction\": feature_fraction,\n",
    "            \"bagging_freq\": 1,\n",
    "            \"bagging_fraction\": bagging_fraction ,\n",
    "            \"bagging_seed\": 11,\n",
    "            \"metric\": 'rmse',\n",
    "            \"lambda_l1\": lambda_l1,\n",
    "            \"verbosity\": -1\n",
    "        }\n",
    "    \n",
    "        clf = lgb.train(param,\n",
    "                        trn_data,\n",
    "                        10000,\n",
    "                        valid_sets = [trn_data, val_data],\n",
    "                        verbose_eval=500,\n",
    "                        early_stopping_rounds = 200)\n",
    "        \n",
    "        oof[val_idx] = clf.predict(train.iloc[val_idx][features],\n",
    "                                   num_iteration=clf.best_iteration)\n",
    "        \n",
    "        del clf, trn_idx, val_idx\n",
    "        gc.collect()\n",
    "        \n",
    "    return -mean_squared_error(oof, target)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bayesian optimization\n",
    "LGB_BO = BayesianOptimization(LGB_CV, {\n",
    "    'max_depth': (4, 10),\n",
    "    'num_leaves': (5, 130),\n",
    "    'min_data_in_leaf': (10, 150),\n",
    "    'feature_fraction': (0.7, 1.0),\n",
    "    'bagging_fraction': (0.7, 1.0),\n",
    "    'lambda_l1': (0, 6)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------\n",
      "|   iter    |  target   | baggin... | featur... | lambda_l1 | max_depth | min_da... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "fold n°0\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's rmse: 1.57404\tvalid_1's rmse: 1.56404\n",
      "[1000]\ttraining's rmse: 1.55286\tvalid_1's rmse: 1.54909\n",
      "[1500]\ttraining's rmse: 1.54259\tvalid_1's rmse: 1.54458\n",
      "[2000]\ttraining's rmse: 1.53524\tvalid_1's rmse: 1.54227\n",
      "[2500]\ttraining's rmse: 1.52914\tvalid_1's rmse: 1.54108\n",
      "[3000]\ttraining's rmse: 1.52346\tvalid_1's rmse: 1.54029\n",
      "[3500]\ttraining's rmse: 1.51819\tvalid_1's rmse: 1.5398\n",
      "[4000]\ttraining's rmse: 1.5133\tvalid_1's rmse: 1.53934\n",
      "[4500]\ttraining's rmse: 1.50863\tvalid_1's rmse: 1.53908\n",
      "[5000]\ttraining's rmse: 1.50409\tvalid_1's rmse: 1.5389\n",
      "[5500]\ttraining's rmse: 1.49969\tvalid_1's rmse: 1.53886\n",
      "Early stopping, best iteration is:\n",
      "[5427]\ttraining's rmse: 1.50031\tvalid_1's rmse: 1.53882\n",
      "fold n°1\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's rmse: 1.57081\tvalid_1's rmse: 1.57924\n",
      "[1000]\ttraining's rmse: 1.54972\tvalid_1's rmse: 1.56341\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/Desktop/fang/elo-cee-690/elo-cee/lib/python3.6/site-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (0.7206662570689866, 0.7193560109628837, 0.7251584987168853, 4.835128628494055, 101.08077241639721, 128.08677817973626)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-95bd1570569d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mLGB_BO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ei'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/fang/elo-cee-690/elo-cee/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTMIZATION_END\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/fang/elo-cee-690/elo-cee/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTMIZATION_STEP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/fang/elo-cee-690/elo-cee/lib/python3.6/site-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-1b9b3cd485dc>\u001b[0m in \u001b[0;36mLGB_CV\u001b[0;34m(max_depth, num_leaves, min_data_in_leaf, feature_fraction, bagging_fraction, lambda_l1)\u001b[0m\n\u001b[1;32m     43\u001b[0m                         \u001b[0mvalid_sets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrn_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                         \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                         early_stopping_rounds = 200)\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         oof[val_idx] = clf.predict(train.iloc[val_idx][features],\n",
      "\u001b[0;32m~/Desktop/fang/elo-cee-690/elo-cee/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    216\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/fang/elo-cee-690/elo-cee/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1800\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1801\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1802\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1803\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('-'*126)\n",
    "\n",
    "start_time = timer(None)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "    LGB_BO.maximize(init_points=2, n_iter=20, acq='ei', xi=0.0)\n",
    "\n",
    "timer(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_submission = pd.read_csv('../data/sample_submission.csv')\n",
    "# sample_submission['target'] = predictions\n",
    "# sample_submission.to_csv('lightGBM_random.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elo-cee",
   "language": "python",
   "name": "elo-cee"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
